#import "./preprint_format.typ": *
#show: preprint.with(
  presentation: [小論文],
  title: "ラスボス撃破時のクライマックス演出を最適化するAI",
  author: "川口 達也",
  university: [],
  faculty: [],
  department: [],
  major: [],
  field: [],
  laboratory: [池田研究室],
  date: ("2025", "6", "1"),
  bibliography-file: "bib/jaist_essay.bib",
)

== これまでの修学内容（専攻を変えた理由）

学部では分子シミュレーションに機械学習を応用するテーマに取り組んだが、卒業研究という性質上、自分で考える範囲が限られていた。
そのため、ゲームAIのような創造性の高い研究室を多く持つJAISTの情報科学分野に魅力を感じ、専攻を変えることを決めた。

= 背景

プレイヤーを没入させるゲームジャンルの一つにロールプレイングゲーム（Role Playing Game: RPG）がある．RPGにはストーリー，探検，クエスト，戦闘など多様な要素が含まれており，プレイヤーはキャラクターに自己を投影し，まるで自分がその世界を生きているかのような没入感@immersion を得られる．
こうした没入感をさらに高めるには，プレイヤーの技量とゲームの挑戦度が程よく釣り合っていることが重要である．プレイヤーがそのような状態に達するとフロー状態@flow に入ることが知られている．しかし，プレイヤーには初心者から熟練者まで幅広い層が存在し，あらかじめ固定された難易度ではすべてのプレイヤーを満足させることは困難である．これに対する解決策として，プレイヤーの状態に応じてゲームの難易度を調整する「動的難易度調整（Dynamic Difficulty Adjustment: DDA）」という手法が提案されている．

一方で，ゲームの体験全体に強く影響する要素として，感動体験が挙げられる．心理学者Daniel Kahneman@Kahneman は，人間は体験の良し悪しを「ピーク時の印象」と「終わり方」で判断する傾向があると述べている．ゲームにおいてもこの傾向は顕著であり，特にエンディング時の演出がゲーム全体の印象に大きな影響を与える．
感動を構成する要素には，ストーリー，グラフィック，音楽などがあるが，本研究ではその中でも音楽に着目する．近年では，ゲーム進行に応じて音楽を変化させる「インタラクティブミュージック」の技術が注目されている．たとえば@iwamoto2017epic では，BGMを複数のパートに分割し，それぞれをループさせながら状況に応じて自然に遷移させることで，音楽が不自然に中断されることなく演出が展開される工夫がなされている．これは，音楽側を調整することでプレイヤー体験を最適化する例である．

しかしながら，音楽に合わせてゲーム側を操作するという逆転のアプローチは，これまでに見られていない．そこで本研究では，あらかじめ用意されたBGMのピークタイミングに合わせて戦闘が終結するよう，ラスボスの行動を制御するエージェントを構築することを提案する．これにより，ゲームのクライマックスにおける音楽とゲームプレイの一体感を高め，プレイヤーに強い感動を与える新しい演出手法を実現することを目指す．

= 関連研究
== 強化学習を用いたRPGのDDA

DDAを実装するためのアプローチはいくつか存在するが，
@rpg_rl_dda では，ターン制バトルゲームにおけるDDAを実現させるため，強化学習が用いられている．
人間対エージェントのターン制バトルとなっており，エージェントはSARSA法（State-Action-Reward-State-Action）という強化学習のアルゴリズムによって，プレイヤーに手ごたえのある戦闘を提供すること目指している．この研究では実際に人間のプレイヤーにゲームをプレイさせ，flow状態を感じることができたという評価を受けている．

== 報酬が周期的に変化する強化学習

関連研究@periodic では、歩行パターンの異なる二足歩行のロボットに対して周期的な報酬関数を組み合わせることで、
単一のフレームワークのみでスキップや走行などの様々な二足歩行動作の効率的に学習を可能にしている。

= 提案手法

本研究では，機械学習モデルとしてDeep Q-Network（DQN）を用いる．DQNは強化学習にディープニューラルネットワークの技術を応用した手法である．
まず，エージェントの行動はプレイヤー，ラスボスに共通しており，攻撃，ため攻撃，回復，攻撃力上昇魔法，防御力上昇魔法の5つとする．
攻撃では技 + 攻撃力値分のダメージを与える．ため攻撃は使用した次のターンにより強いダメージを与える．
攻撃力上昇，防御力上昇は3ターン継続とする．また，行動にはそれぞれアニメーションにかかる時間の情報を含める．
次に，状態は最大HP，現在のHP，攻撃力，防御力，ループまでの終了時間の5つの情報をもつ．最大HPはプレイヤー，ラスボス共に100とし，
現在のHP，攻撃力，防御力，ループまでの終了時は正規化する．
また，報酬関数はBGMに基づいて決定する．BGMの設定について，BPM（beats per minute）は120で固定し，ループの長さは8，12，16小節の3パターンを用いる．ただし，1小節とは4拍分の長さのことである．
ループが終了するタイミングに近いほど点数が高くなるように報酬関数を設計する．以下に本研究で提案する報酬関数の概略図を示す．ただし，学習を安定させるため連続的な関数とする．

さらに、学習初期における安定化のため、追加の中間報酬を導入する。
これは、スクリプトAIや人間による模擬プレイデータから平均化した「ラスボスのHP残量と経過時間の関係」を基準とし、実際のHP残量との差に応じて減点するものである。
ただし、本研究の目的は「ループ終了時に戦闘を終えること」であるため、途中の中間報酬の割引率は低く設定する。

また，行動選択には $epsilon$ -greedy法を用いる．これは $epsilon$ の確率でランダムな行動を選択し，$1-epsilon$ の確率でQ値が最大となる行動を選択するアルゴリズムである．

続いて，ゲームの評価方法については，学習後のエージェントによる討伐時間のヒストグラムを作成し，ループ終了タイミングとの一致度を定量的に評価することに加え，実際のプレイヤーに感動体験に関する5段階評価のアンケートを実施し，主観的満足度を測定する．


// = 課題

= 展望

本研究は，ゲームAIが演出と連携しプレイヤー体験を動的に最適化する新しい可能性を提示するものである．音楽に合わせてAIがゲーム進行を調整するという点が本研究の独自性であり，今後は複数キャラクターによるパーティバトルや，アクションゲームなどのリアルタイム性の高いジャンルへの応用も期待される．