#import "./preprint_format.typ": *

#show: preprint.with(
  presentation: [口頭発表],
  title: "RPGのラスボス撃破時におけるクライマックス演出の最適化に関する研究",
  author: "川口 達也",
  university: [],
  faculty: [],
  department: [],
  major: [],
  field: [],
  laboratory: [],
  date: ("2025", "7", "5"),
  bibliography-file: "bib/jaist_essay.bib",
)

= 初めに
それでは、RPGのラスボス撃破時におけるクライマックス演出の最適化に関する研究と題しまして、口頭発表の方を始めさせていただきます。

= 専攻を変えた理由
まず初めに、学部での研究内容と、専攻を変えようと思った理由について簡単に説明します。
学部では分子シミュレーションに機械学習を応用する研究に取り組みました。
しかし、限られた期間では理論を理解するのに精一杯で、機械学習のモデル設計まで深く踏み込むことができなかったことが心残りでした。
そのため、大学院では最低でも機械学習を使った研究に取り組みたいと考えていました。
そうして研究室を探す中で、ゲームAIを扱う興味深い研究室を見つけたので、もともとゲームにも関心があったことから、JAISTの情報科学分野を志望いたしました。

= 研究背景
次に、JAISTで取り組みたい研究について説明いたします。本研究の目的は、RPGのラスボス戦において、機械学習を用いて演出を最適化し、すべてのプレイヤーにとって感動的な体験となるクライマックスを実現することです。

続いて研究背景です。
プレイヤーを没入させるゲームジャンルの一つにロールプレイングゲームが挙げられます。（以下ではRPGと呼びます）
RPGにはストーリーや冒険、戦闘など様々な要素があり、プレイヤーはキャラクターに自己を投影し、まるで自分がその世界を生きているかのような没入感を得ることができます。

心理学者のダニエル・カーネマンによれば、人間は過去の体験を振り返るとき、「ピークの瞬間」と「終わり方」の印象によって、その体験全体の評価を決める傾向があるとされています。
これはゲームにおいても顕著であり、RPGにおいては特にラスボス戦における体験がそのゲーム全体の印象に大きく影響します。
本研究では、ラスボス戦の感動体験の要素として、没入感と音楽演出に着目しました。

まず没入感についてです。
没入感を高めるためには、プレイヤーの技量とゲームの難易度のバランスが重要です。
難易度がプレイヤーにとって、簡単すぎず難しすぎないちょうどいいバランスになっていると、プレイヤーはフロー状態と呼ばれる状態に入り、プレイヤーに強い没入感を体験させることができます。
ですが、プレイヤーには初心者から熟練者まで幅広い層が存在し、あらかじめ固定された難易度では、すべてのプレイヤーを満足させることは難しいです。
この課題に対するアプローチとして、プレイヤーの状態に応じてゲームの難易度を調整する、動的難易度調整という手法が提案されています。（以下ではDDAと呼びます）

続いて音楽演出についてです。
ラスボス戦における音楽は、プレイヤーの感情に直接働きかける重要な要素です。
近年では、ゲームの状況に応じて音楽を変化させるインタラクティブミュージックという技術が注目されています。
この技術自体は昔から研究されており、
// （例えばスーパーマリオブラザーズにおけるテンポアップや、ゼルダの伝説シリーズのバトルシーン移行時のシームレスな曲調の変化などが挙げられます。）
近年ではFF15のボス戦において、メインパートとエンドパートの間にプレエンドパートを設け、そのパートではループ間隔を短くすることで、いつでもエンドパートに遷移できる工夫がされています。こうした工夫によって、ボス戦のBGMがそのまま自然に勝利BGMへと移る演出が可能になります。

このようにインタラクティブミュージックという分野では、ゲームの状況に応じて音楽を変化させる取り組みが進められてきました。しかしその一方で、音楽に合わせてゲームの状況を変化させるという逆のアプローチはまだあまり行われていません。そこで本研究では、BGMのクライマックスに合わせてちょうどラスボスが倒されるように、ラスボスの行動を制御するAIを構築します。

以上のように、本研究では没入感と音楽演出の二つの要素に着目し、すべてのプレイヤーにとって手ごたえがあり、感動的なクライマックスを演出するラスボス戦の実現を目指します。
（4分）（タイトル含め8枚）

= 研究手法

本研究では、ドラクエやFFなどに代表される、一般的なターン制のコマンド型RPGを対象とします。
また、機械学習の手法としては、強化学習を行います。
強化学習とは、エージェントが環境の中で行動し、その結果得られる報酬をもとに、「どのように行動すれば最適かを学習する機械学習の一種です。
たとえば囲碁を例にすると、環境はゲームのルールと盤面、エージェントは対戦プレイヤーに相当します。
対戦プレイヤーAIは、状態と呼ばれる、現在の盤面の碁石の配置を見ながら、行動と呼ばれる、次にどこに石を置くかを選びます。
そして、対局を通じて得られる勝敗などの報酬をもとに、AIがプレイヤーのように経験を積みながら、より良い行動の選び方を自ら学んでいくのが、強化学習の特徴です。
本研究では、強化学習アルゴリズムとしてDeep Q-Networkを使います。（以下ではDQNと呼びます）
DQNは強化学習にニューラルネットワークの技術を応用した手法です。

続いてゲームの設定について説明します。
一般的なコマンドRPGゲームを参考にし、状態には、～、行動には、～を定義します。

BGMはループの一番最後に最も盛り上がると仮定し、それに基づいて報酬関数を設定します。
報酬関数はこのグラフのように、BGMのループの終盤でラスボスが討伐されるほど、点数が高くなるように設計します。

また、討伐までの学習を安定化させるために、中間報酬も用意します。
簡単なスクリプトAIから、経過時間とラスボスのHP残量のグラフをあらかじめ作成します。
だいたいこのような右下がりのグラフになると思います。
この理想のHP残量と実際のHP残量を比較し、その差分を減点することで討伐まで順調にラスボスのHPが減らせているかを判断します。
ただし、BGMの終わりに倒されることが優先なので、この中間報酬は点数を低めにしておきます。

続いて、さらにプレイヤーを熱くさせる要素として、攻撃の命中や回避、クリティカルと呼ばれたりする倍以上の攻撃などの運要素を取り入れ、これらを意図的に確率操作することをします。
例えば終盤ギリギリのところでクリティカルを出して倒す、みたいなことです。

続いて、結果の評価方法について説明します。
学習後のエージェントについて、討伐時間のヒストグラムを作成し、ループ終了タイミングとの一致度を定量的に評価します。
また、実際に被験者に、単純なスクリプトAIのラスボスと学習済みのエージェントのラスボスの両方と対戦してもらい、どちらが面白かったか、熱くなったかなどの項目でアンケートを受けてもらい、主観的な満足度についても調査を行います。

最後に課題及び今後の展望について考察します。
本研究は、ゲームAIが演出と連携し、プレイヤー体験を最適化する新しい可能性を提示する研究です。
課題としては、なかなかちょうどいいタイミングでプレイヤーがコマンドを選択してくれない場合、戦闘が終結せずグダってしまうことや、〜などが考えられます。
今後の展望としては、複数キャラクターによるパーティバトルや、アクションゲームなどのリアルタイム性の高いジャンルへの応用なども期待できます。
以上で発表を終わります。
ご清聴ありがとうございました。
（3分）（7枚）
