#import "./preprint_format.typ": *

#show: preprint.with(
  presentation: [口頭発表],
  title: "RPGのラスボス撃破時におけるクライマックス演出の最適化に関する研究",
  author: "川口 達也",
  university: [],
  faculty: [],
  department: [],
  major: [],
  field: [],
  laboratory: [],
  date: ("2025", "7", "5"),
  bibliography-file: "bib/jaist_essay.bib",
)

= 初めに
それでは、RPGのラスボス撃破時におけるクライマックス演出の最適化に関する研究と題しまして、口頭発表の方を始めさせていただきます。

= 専攻を変えた理由
まず初めに、学部での研究内容と、専攻を変えようと思った理由について簡単に説明します。
学部では分子シミュレーションに機械学習を応用する研究に取り組みました。
しかし、限られた期間では理論を理解するのに精一杯で、機械学習のモデル設計まで深く踏み込むことができなかったことが心残りでした。
そのため、大学院では最低でも機械学習を使った研究に取り組みたいと考えていました。
そうして研究室を探す中で、ゲームAIを扱う興味深い研究室を見つけたので、もともとゲームにも関心があったことから、JAISTの情報科学分野を志望いたしました。

= 研究背景
次に、JAISTで取り組みたい研究について説明いたします。本研究の目的は、ターン制のコマンド型RPGのラスボス戦において、機械学習を用いて演出を最適化し、プレイヤーに感動を与えるラスボス戦を提供することです。

続いて研究背景です。
プレイヤーを没入させるゲームジャンルの一つにロールプレイングゲームが挙げられます。（以下ではRPGと呼びます）
RPGにはストーリーや冒険、戦闘など様々な要素があり、プレイヤーはキャラクターに自己を投影し、まるで自分がその世界を生きているかのような没入感を得ることができます。

こうした没入感をさらに高めるためには、プレイヤーの技量とゲームの難易度のバランスが重要です。
難易度がプレイヤーにとって、簡単すぎず難しすぎないちょうどいいバランスになっていると、プレイヤーはフロー状態と呼ばれる状態に入り、プレイヤーに強い没入感を体験させることができます。
ですが、プレイヤーには初心者から熟練者まで幅広い層が存在し、あらかじめ固定された難易度では、すべてのプレイヤーを満足させることは難しいです。
これに対する解決策として、プレイヤーの状態に応じてゲームの難易度を調整する、動的難易度調整という手法が提案されています。（以下ではDDAと呼びます）

ダニエルカーネマンの「エバリュエーションバイモーメンツパストアンドフューチャー」によれば、人間は体験の評価を「ピーク時の印象」と「終わり方」によって判断する傾向があります。
これはゲームにおいても顕著で、エンディング時の感動がそのゲーム全体の印象に大きく影響します。
感動の構成要素にはストーリーやグラフィック、音楽などが挙げられますが、本研究では音楽に主に着目します。

ラスボス戦における音楽は、プレイヤーの感情に直接働きかける重要な要素です。
近年では、ゲームの状況に応じて音楽を変化させるインタラクティブミュージックという技術が注目されています。
この技術自体は昔からあり、例えばスーパーマリオブラザーズにおけるテンポアップや、ゼルダの伝説シリーズのバトルシーン移行時のシームレスな曲調の変化などが挙げられます。
最近ではFF15において、音楽を楽曲のトラックをリアルタイムで変化させたり、フレーズやセクションを再構成することで、FFの印象的なメロディを保ちつつ、音楽を自然に変化させる工夫がされています。

このようにインタラクティブミュージックという分野では、ゲームの状況に応じて音楽を変化させる取り組みが行われてきましたが、逆に、音楽に合わせてゲームの状況を変化させる取り組みはまだあまり行われていません。
そこで本研究では、BGMのクライマックスに合わせてラスボスが倒されるように、ラスボスの行動を制御するAIを構築することで、ラスボス戦における感動的なクライマックス演出の最適化を実現することを目的とします。
（4分）（タイトル含め8枚）

= 研究手法

// 本研究では、機械学習モデルとしてDeep Q-Networkを使います。（以下ではDQNと呼びます）
// DQNは強化学習にニューラルネットワークの技術を応用した手法です。
強化学習とは、エージェントが環境の中で行動し、その結果として得られる報酬をもとに、どう行動すれば最適かを学習する機械学習の方法の一つです。
たとえばトランプゲームを例にすると、環境は場に出ているカード、エージェントは強くしたい対戦プレイヤーにあたります。
対戦プレイヤーAIは、状態と呼ばれる、手札や場の状況を見ながら行動を選び、どうすれば勝ちやすくなるかを試行錯誤しながら学習していきます。
このように、強化学習では、AIがプレイヤーのように経験を積みながら、より良い行動の選び方を自分で身につけていくのが特徴です。

一般的なコマンドRPGゲームを参考にし、状態には、～、行動には、～を定義します。

BGMはループの一番最後に最も盛り上がると仮定し，それに基づいて報酬関数を設定します．
報酬関数はこのグラフのように、BGMのループの終盤でラスボスが討伐されるほど、点数が高くなるように設計します。

また、討伐までの学習を安定化させるために、中間報酬も用意します。
簡単なスクリプトAIから、経過時間とラスボスのHP残量のグラフをあらかじめ作成します。
だいたいこのような右下がりのグラフになると思います。
この理想のHP残量と実際のHP残量を比較し、その差分を減点することで討伐まで順調にラスボスのHPが減らせているかを判断します。
ただし，BGMの終わりに倒されることが優先なので，この中間報酬は点数を低めにしておきます．

続いて、さらにプレイヤーを熱くさせる要素として、攻撃の命中や回避、クリティカルと呼ばれたりする倍以上の攻撃などの運要素を取り入れ、これらを意図的に確率操作することをします。
例えば終盤ギリギリのところでクリティカルを出して倒す，みたいなことです．

続いて，結果の評価方法について説明します．
学習後のエージェントについて，討伐時間のヒストグラムを作成し，ループ終了タイミングとの一致度を定量的に評価します．
また，実際に被験者に，単純なスクリプトAIのラスボスと学習済みのエージェントのラスボスの両方と対戦してもらい，どちらが面白かったか，熱くなったかなどの項目でアンケートを受けてもらい，主観的な満足度についても調査を行います．

最後に課題及び今後の展望について考察します．
本研究は，ゲームAIが演出と連携し，プレイヤー体験を最適化する新しい可能性を提示する研究です．
課題としては，なかなかちょうどいいタイミングでプレイヤーがコマンドを選択してくれない場合，戦闘が終結せずグダってしまうことや，〜などが考えられます．
今後の展望としては，複数キャラクターによるパーティバトルや，アクションゲームなどのリアルタイム性の高いジャンルへの応用なども期待できます．
以上で発表を終わります．
ご清聴ありがとうございました．
（3分）（7枚）
