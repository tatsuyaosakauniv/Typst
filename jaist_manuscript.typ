#import "./preprint_format.typ": *

#show: preprint.with(
  presentation: [口頭発表の原稿],
  title: "RPGのボス撃破時におけるクライマックス演出の最適化に関する研究",
  author: "川口 達也",
  university: [],
  faculty: [],
  department: [],
  major: [],
  field: [],
  laboratory: [],
  date: ("2025", "7", "5"),
  bibliography-file: "bib/jaist_essay.bib",
)

= タイトル
それでは，RPGのボス撃破時におけるクライマックス演出の最適化に関する研究と題しまして，口頭発表の方を始めさせていただきます．

= 研究背景
まずは研究背景についてご説明いたします．
まずゲームAIと聞かれると，チェスや囲碁などで人間に勝つために作られるものを連想しがちですが，必ずしも強さだけを追求するわけではありません．
近年では，プレイヤーを楽しませてくれるように手加減するAIの研究が注目されています．
例えば，プレイヤーにとって強すぎず弱すぎない手を打つ囲碁AIの研究例もあります．
このように，面白さを追求するためのAI研究は盛んにおこなわれています．

= 研究背景
一方で，感動的な体験もまたゲームの面白さを構成する重要な要素です．
// プレイヤーの記憶に残るような緊張感や達成感の演出は，ゲーム全体の評価や満足度に大きく影響します．
例えば、人の感情を顔の表情や脈拍といった情報で判断する研究例があります。
しかし、感動体験そのものをAIによって最適化する研究は，まだ十分に行われているとは言えません．

= 研究目的
そこで本研究では，強化学習を用いてゲーム演出を心理的に最適化することで，プレイヤーに感動的な体験を提供することを目的とします．
本研究では，感情的な展開の制御と，音楽に同期するためのAI制御の2つに着目します．
詳細については後ほどご説明いたします。

= 研究手法｜ゲームのテーマ
続いて，本研究で扱うゲームについてご説明いたします．
本研究では，「コマンド選択型のRPGにおけるボス戦」を題材とします．
この題材を選定した理由は，大きく3つあります．
1つ目は，RPGが没入感を重視するゲームジャンルであり，感動体験との親和性が高いから．
2つ目は，コマンド選択型のゲームは状態が不連続であり，強化学習の設計が比較的シンプルで扱いやすいと考えられるから．
3つ目は，ボス戦がゲーム全体を通して重要な局面であり，プレイヤーの記憶に強く残るからです．

= 研究手法｜ゲーム制作
続いて，具体的な研究手法についてご説明いたします．
本研究では、RPGを自作して扱おうと思っています。
その理由は、機械学習と連携できるRPGのシミュレータが

本研究では、強化学習アルゴリズムとしてPPOを用います。
// 機械学習の方はpythonで処理を行います．
// 本研究では，強化学習アルゴリズムとしてDeep Q-Networkを使います．（以下ではDQNと呼びます）
// DQNは強化学習にニューラルネットワークの技術を応用した手法です．

= 研究手法｜強化学習
続いて、強化学習について簡単に説明します。
強化学習とは、ゲームAIなどによく用いられる機械学習の一種です。
ここでは、その仕組みをわかりやすく説明するために、犬のしつけを例に取ります。
例えば、犬に「お座り」を覚えさせたいとします。
犬は最初、さまざまな行動を自由に行いますが、「立っている状態からお座りをしたとき」にだけ、報酬として餌を与えます。
すると，犬はやがて、餌をたくさんもらうために、お座りの行動を学習していくようになります。
このように、AIも報酬をできるだけ多く得るために、試行錯誤を繰り返しながら、どのような行動をとるべきかを学習していきます。

= 目次
続いて，感動演出のための，具体的な強化学習手法について説明いたします．

= 研究手法｜感情的展開の制御
まずは，強化学習を用いて，感情的な展開を制御する方法について説明します．
初期段階では，「HPギリギリで耐える」や「会心の一撃でボスを倒す」といった感情を動かすようなイベントを複数設定し，ボス戦中にランダムに発生させます．
被験者にはこれらのイベントを含むRPGをプレイしてもらい，終了後に印象的だった場面とそのときの感情についてアンケートに答えてもらいます．
あわせて，選択したコマンドやその傾向，選択にかかった時間，待機中のボタン操作などのプレイログも記録します．
アンケート結果に基づき，好意的に受け取られた場面は報酬に加点し，逆に否定的な印象を与えた場面では減点します．
このようにして得られたデータをもとに，プレイヤーの感情に応じて最適なイベントを選択できるよう，ボスの行動を強化学習で学習させます．
将来的には，表情や音声などのマルチモーダルを用いて，プレイヤーの感情を推定するモデルを構築し，指定した感情曲線に合わせて展開を制御することができれば面白いと考えています．

// そのほかの点としては，攻撃の命中や回避，確率的に発生する高威力な攻撃，いわゆる会心の一撃などの運要素を意図的に確率操作することなども考えています．

= 研究手法｜音楽演出の最適化
続いて，音楽演出についてご説明いたします．
ボス戦の終わり方は、ゲーム体験の印象を大きく作用します。
特に、BGMが戦闘の決着と同時に自然に終わり、勝利BGMへとスムーズにつながることで、プレイヤーの没入感が維持されます。
そこで、本研究では、先ほど説明した感情的な展開の制御に加え、音楽演出の最適化にも注目します。

= 研究手法｜関連研究
ゲーム音楽は映画音楽などと異なり，ループする点が特徴です．
プレイ時間がプレイヤーに委ねられているため，音楽が途中で途切れると違和感を生じさせてしまいます．
そこで1990年頃からは，ゲームの状況に応じて音楽を変化させる「インタラクティブミュージック」という技術が注目され，音楽を自然につなげる工夫が進められてきました．
例えば2017年に発表されたff15では，ボス戦のBGMにおいて，メインパートとエンドパートの間にプレエンドパートを設け，このパートではループ間隔を短くすることで，いつでも自然にエンドパートへ遷移しやすくする工夫がされています。
こうすることで，戦闘終了と音楽の終わりが一致し、勝利BGMへと滑らかにつながる演出が可能になります．
しかし，同じような構成をもつ曲が増えるという問題も挙げられます．
このように従来のインタラクティブミュージックでは，ゲームの状況に応じて音楽を変化させる取り組みが進められてきましたが，音楽に合わせてゲームの状況を変化させるという逆のアプローチはまだあまり行われていません．

= 研究手法｜BGMと報酬
そのため，本研究では，曲のテンポや長さを変化させることは行わず，あらかじめ決められたBGMに合わせて，ゲーム側を制御します．
本研究では，報酬をこのような関数に基づいて決定します。
灰色の縦の点線はループが終了するタイミングを示していて、ループの終盤付近でボスを倒すほど報酬を高く与えます。
こうすることで，ループ終了の手前でボスが倒されて，違和感なく勝利BGMに遷移できると考えています．
// また，状態にはBGMのループ時間を1となるように時間を正規化し，現在の時刻を0～1の数値として状態に含めることで，今がループのどのあたりかエージェントが把握できるようにします．（ここジェスチャー）

将来的には，〜などの応用も期待できると思っています．
ご清聴ありがとうございました．

= 計画

// 予備スライドで説明するかも
// 続いて，リアルタイムで勝敗予測を行います．これによって，プレイヤーの極端に下手な行動に対しては，容赦なく攻撃することで，無粋な手加減を防ぎます．


// まるまるカット
// = 専攻を変えた理由
// まず初めに，学部での研究内容と，専攻を変えようと思った理由について簡単に説明します．
// 学部では分子シミュレーションに機械学習を応用する研究に取り組みました．
// しかし，限られた期間では理論を理解するのに精一杯で，機械学習のモデル設計まで深く踏み込むことができなかったことが心残りでした．
// そのため，大学院では最低でも機械学習を使った研究に取り組みたいと考えていました．
// そうして研究室を探す中で，ゲームAIを扱う興味深い研究室を見つけたので，もともとゲームにも関心があったことから，JAISTの情報科学分野を志望いたしました．
// ここまで